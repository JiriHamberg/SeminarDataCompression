% --- Template for thesis / report with tktltiki2 class ---
% 
% last updated 2013/02/15 for tkltiki2 v1.02

\documentclass[english]{tktltiki2}

% tktltiki2 automatically loads babel, so you can simply
% give the language parameter (e.g. finnish, swedish, english, british) as
% a parameter for the class: \documentclass[finnish]{tktltiki2}.
% The information on title and abstract is generated automatically depending on
% the language, see below if you need to change any of these manually.
% 
% Class options:
% - grading                 -- Print labels for grading information on the front page.
% - disablelastpagecounter  -- Disables the automatic generation of page number information
%                              in the abstract. See also \numberofpagesinformation{} command below.
%
% The class also respects the following options of article class:
%   10pt, 11pt, 12pt, final, draft, oneside, twoside,
%   openright, openany, onecolumn, twocolumn, leqno, fleqn
%
% The default font size is 11pt. The paper size used is A4, other sizes are not supported.
%
% rubber: module pdftex

% --- General packages ---

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{amsfonts,amsmath,amssymb,amsthm,booktabs,color,enumitem,graphicx}
\usepackage[pdftex,hidelinks]{hyperref}

%\usepackage{cleveref}

% Automatically set the PDF metadata fields
\makeatletter
\AtBeginDocument{\hypersetup{pdftitle = {\@title}, pdfauthor = {\@author}}}
\makeatother

% --- Language-related settings ---
%
% these should be modified according to your language

% babelbib for non-english bibliography using bibtex
\usepackage[fixlanguage]{babelbib}

% add bibliography to the table of contents
\usepackage[nottoc]{tocbibind}

\usepackage[outputdir=build]{minted}

% --- Theorem environment definitions ---

\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}


% --- tktltiki2 options ---
%
% The following commands define the information used to generate title and
% abstract pages. The following entries should be always specified:

\title{Data Compression Seminar: wfLZ}
\author{Jiri Hamberg}
\date{\today}
\level{Seminar report}
%\abstract{Abstract.}

% The following can be used to specify keywords and classification of the paper:

\keywords{keyword 1, keyword 2, keyword 3}

% classification according to ACM Computing Classification System (http://www.acm.org/about/class/)
% This is probably mostly relevant for computer scientists
% uncomment the following; contents of \classification will be printed under the abstract with a title
% "ACM Computing Classification System (CCS):"
% \classification{}

% If the automatic page number counting is not working as desired in your case,
% uncomment the following to manually set the number of pages displayed in the abstract page:
%
% \numberofpagesinformation{16 pages + 10 appendix pages}
%
% If you are not a computer scientist, you will want to uncomment the following by hand and specify
% your department, faculty and subject by hand:
%
% \faculty{Faculty of Science}
% \department{Department of Computer Science}
% \subject{Computer Science}
%
% If you are not from the University of Helsinki, then you will most likely want to set these also:
%
% \university{University of Helsinki}
% \universitylong{HELSINGIN YLIOPISTO --- HELSINGFORS UNIVERSITET --- UNIVERSITY OF HELSINKI} % displayed on the top of the abstract page
% \city{Helsinki}
%


\begin{document}

% --- Front matter ---

\frontmatter      % roman page numbering for front matter

\maketitle        % title page
%\makeabstract     % abstract page

\tableofcontents  % table of contents

% --- Main matter ---

\mainmatter       % clear page, start arabic page numbering

\section{Introduction}

wfLZ is an open source Lempel-Ziv compressor developed by Shane Calimlim. The compressor is a C library that is, according to the author, designed for use in game engines with focus in cross platform compatibility. The design is very minimalistic - the entire source code of the library consists of 838 lines of code and one C header file of 144 lines. The compressor has two compression levels, referred here as "fast" and "slow" for clarity. When using the "slow" compression level, the compressor trades compression speed for additional compression ratio. Both algorithms are discussed in detail in section 3.  

A notable feature of the wfLZ compressor is that unlike most compressors of the Squash Compression Benchmark, it operates in memory. To achieve streaming to a file, one must utilize some form of memory-to-file mapping, such as \textit{mmap} on Unix-like systems or \textit{File Mapping} on Windows systems, but the library itself offers no direct support for this.

The wfLZ library also has some utility functions for supporting a compression mode called \textit{chunked} compression where the input is divided to equal sized chunks which can be compressed concurrently or in parallel. The API does not exactly implement a parallel compressor but rather offers tools utilities for dividing the data in chunks allowing the user to implement a parallel version of the compressor. This design choice is probably due to trying to keep the project as cross platform compatible as possible. As such, I will not be discussing the chunked compressor mode any further in this report. 

\section{Performance Evaluation}

\begin{figure}
	\includegraphics[width=1.0\textwidth]{../img/dickens_edited.png}
	\caption{
		\textit{dickens}-dataset from the Squash Compression Benchmark. Collected works of Charles Dickens, 9.72 MiB. The wfLZ data points are circled with red. The data points corresponding to the "fast" and "slow" compressor levels are labelled with the words \textit{fast} and \textit{slow} respectively and the data points corresponding to "chunked" compression are labelled with the word \textit{chunked}. 
	}
	\label{fig:dickens}
\end{figure}


\begin{figure}
	\includegraphics[width=1.0\textwidth]{../img/c_source_edited.png}
	\caption{
		\textit{fields.c}-dataset from the Squash Compression Benchmark. Small C-source code of 10.89 KiB. The wfLZ data points are circled with red. The data points corresponding to the "fast" and "slow" compressor levels are labelled with the words \textit{fast} and \textit{slow} respectively and the data points corresponding to "chunked" compression are labelled with the word \textit{chuncked}. 
	}
	\label{fig:csource}
\end{figure}

\begin{figure}
	\includegraphics[width=1.0\textwidth]{../img/wikipedia_decompression.png}
	\caption{
		\textit{enwiki8}-dataset from the Squash Compression Benchmark. First $10^8$ bytes, or 95.37 MiB from the Wikipedia dump. The wfLZ data points are circled with red. The data points corresponding to the "fast" and "slow" compressor levels are labelled with the words \textit{fast} and \textit{slow} respectively and the data points corresponding to "chunked" compression are labelled with the word \textit{chunked}.
	}
	\label{fig:wikipedia}
\end{figure}

The performance of the wfLZ compressor does not stand out favourably in the Squash Compression Benchmark. While using the "fast" compression level, the compression speed is reasonably fast, the compression ratio it achieves is amongst the worst of the compressors with comparable compression speed.

When using the "slow" compression level, the wfLZ compressor is amongst the slowest if not the slowest compressor in most cases. This is made worse by the fact that the compression ratio achieved by with the slow compression level is also the lowest amongst the compressors with comparable compression speeds. In fact, this inequality can be highlighted by choosing a dataset where the "slow" compression algorithm fairs especially poorly, such as the \textit{dickens}-dataset (Collected works of Charles Dickens with combined size of 9.72 MiB). Using this dataset the wfLZ achieved compression speed of only 19.5Kib/s and a compression ratio of 1.71, whereas the fastest compressor using this dataset, density, achieved compression speed of 227.75Mib/s and a very much comparable compression ratio of 1.75. That is a whopping 4 orders of magnitude difference in the compression speed. I will discuss the source of this incredibly poor performance in section 3. Figures~\ref{fig:dickens},\ref{fig:csource} illustrate typical compression speed and compression ratio for the wfLZ compressor with different compressor settings.      

The only metric in the Squash Compression Benchmark where the wfLZ really shines is the decompression speed, where the wfLZ consistently ranks within the top 5 decompressors. This is mostly due to the very simple compression format that the wfLZ uses. The fact that the wfLZ compressor operates in memory might also give it an advantage in the benchmark when compared to decompressors that implement actual file I/O. Figure~\ref{fig:wikipedia} illustrates a typical decompression vs. compression-ratio trade off for the wfLZ compressor.



\section{Compression Format}

\section{Compression}

The wfLZ compressor is a very simple Lempel-Ziv compressor that uses a small hashtable to find matching subsections from the already traversed part of the input. The hashtable size is by default 256 KB, which fits entirely in an L2-cache of most modern CPUs. The hashtable considers 4 bytes of the input at a time, hashing these bytes to an unsigned 32 bit integer.

The following section of code (slightly simplified by removing logging and endianess-handling) shows the core of the compression algorithm. First, the hashtable is checked for a match. Then, the match is validated by requiring it to be longer than the block size and requiring the distance to the match to fit in a 16-bit unsigned integer. If a valid match was found, a new block will be emitted. Otherwise a literal will be emitted and in case the current block has maximum number of literals already, a new block will be emitted first.    

\begingroup
\catcode`\-=11
\begin{minted}[tabsize=2, breaklines]{c}
while( bytesLeft >= WFLZ_MIN_MATCH_LEN )
{
	const uint32_t hash = WFLZ_HASHPTR( src );
	const uint8_t** dictEntry = &dict[ hash ].inPos;
	const uint8_t* matchPos = *dictEntry;
	const uint8_t* windowStart = src - WFLZ_MAX_MATCH_DIST_FAST;
	ureg_t matchLength = 0;
	const ureg_t maxMatchLen = WFLZ_MAX_MATCH_LEN > bytesLeft ? bytesLeft : WFLZ_MAX_MATCH_LEN ;
	*dictEntry = src;
	if( matchPos != NULL && matchPos >= windowStart )
	{
		matchLength = wfLZ_MemCmp( src, matchPos, maxMatchLen );
	}
	if( matchLength >= WFLZ_MIN_MATCH_LEN )
	{
		const uint32_t matchDist = src - matchPos;
		block->numLiterals = ( uint8_t )numLiterals;
		block = ( wfLZ_Block* )dst;
		bytesLeft -= matchLength;
		dst += WFLZ_BLOCK_SIZE;
		src += matchLength;
		block->dist = ( uint16_t )matchDist;
		block->length = ( uint8_t )( matchLength - WFLZ_MIN_MATCH_LEN + 1 );
		numLiterals = 0;
		header.compressedSize += WFLZ_BLOCK_SIZE;
	}
	else
	{
		if( numLiterals == WFLZ_MAX_SEQUENTIAL_LITERALS )
		{
			block->numLiterals = ( uint8_t )numLiterals;
			block = ( wfLZ_Block* )dst;
			dst += WFLZ_BLOCK_SIZE;
			block->dist = block->length = 0;
			numLiterals = 0;
			header.compressedSize += WFLZ_BLOCK_SIZE;
		}
		++numLiterals;
		--bytesLeft;
		*dst++ = *src++;
		++header.compressedSize;
	}
}				
\end{minted}
\endgroup

This functionality is the core of both "fast" and "slow" compressors. The "slow" compressor however adds an extra step to the match finding logic to increase compression ratio at the cost of compression speed. Instead of simply validating the hashtable match, the "slow" compressor iterates over the range $[\textit{currentPos} - \textit{maxMatchDist}, \textit{hashtable[ hash(currentPos) ]}]$ looking for the best match. This method is guaranteed to find the best match within the allowed distance (16-bit unsigned integer). Note that there cannot be any hash collisions in range $[ \textit{hashtable[hash(currentPos)]}, \textit{currentPos} ]$, because then the value of expression $\textit{hashtable[hash(currentPos)]} ]$ would in fact be the location of the last of such collision.

The downside of this approach is the decrease in compression speed. It is easy to see that in the worst case, the algorithm will have to traverse \textit{maxMatchDist} (65535) bytes in almost every position of the input. What is worse, is that the input pattern that leads to terrible performance seems to be quite common in real world: in fact, all that is required is that for most input positions a (sufficiently long) match is found in a nearby location. If we consider literature as an example, we can see that it reflects this requirement quite well: many words are contextual in the sense that they are likely to appear multiple times within a relatively short section of the input and most words will not be unique in the input. This explains why the wfLZ "slow" compressor is four orders of magnitude slower than the fastest compressor with comparable compression ratio in Squash Compression Benchmark with the \textit{Collected Works of Charles Dickens} -dataset.

\section{Decompression}

% Write some science here.

%Sample text and a reference~\cite{example}.


% --- References ---
%
% bibtex is used to generate the bibliography. The babplain style
% will generate numeric references (e.g. [1]) appropriate for theoretical
% computer science. If you need alphanumeric references (e.g [Tur90]), use
%
% \bibliographystyle{babalpha-lf}
%
% instead.

%\bibliographystyle{babplain-lf}
%\bibliography{references-en}


% --- Appendices ---

% uncomment the following

% \newpage
% \appendix
% 
% \section{Example appendix}

\end{document}